# Browser-Use Project Rules for AI Coding Assistant

## Project Context
This is a Tesla sentiment analysis project using browser-use (https://github.com/browser-use/browser-use) for web scraping automation with LLM-powered agents.

## 🎯 Quick Start Summary (AI: Read This First)

### Golden Rules for Reliable Browser-Use Agents:
1. **Use Direct URLs** - Navigate to pre-built URLs with embedded queries, not multi-step flows
2. **Extract First, Scroll Later** - Collect visible content before attempting any scrolling
3. **Limit Actions** - max_actions_per_step ≤ 6 to avoid scroll timeouts
4. **Cookie-Based Login** - Manual cookie export > automated browser login
5. **Stay On Target** - Explicitly forbid search engines in "CRITICAL RULES" section
6. **Wait Generously** - 10s page loads, 6s between scrolls, 2s minimum between actions
7. **Accept Partial Results** - Return what you have if scrolling fails
8. **Validate Everything** - Check cache structure, URL domains, timestamp formats

### Common Timeout Fixes:
- Screenshot timeout → use_vision=False
- Scroll timeout → max_actions_per_step=6, wait 6s after scrolls
- Page load timeout → minimum_wait_page_load_time=2.0+

### When Agent Fails:
1. Check logs for "Searched Duckduckgo" → add CRITICAL RULES to prompt
2. Check for scroll timeouts → use extract-first strategy
3. Check for empty output → verify output_model_schema is set
4. Check cache issues → validate cache structure and expiration

## Official Browser-Use Resources (AI: Reference These)
- **GitHub**: https://github.com/browser-use/browser-use
- **Examples**: https://github.com/browser-use/browser-use/tree/main/examples
- **Local Examples**: `.browser-use-reference/examples/` (cloned official examples)
- **Quick Reference**: `examples/QUICK_REFERENCE.md`
- **Production Examples**: `app/adapters/twitter_source.py`, `app/adapters/reddit_source.py`

## Core Browser-Use Patterns (AI: Follow These)

### 1. Agent Creation Pattern
```python
from browser_use import Agent, ChatOpenAI
from browser_use.browser.profile import BrowserProfile
from pydantic import BaseModel

# Standard agent setup
llm = ChatOpenAI(
    model=settings.planner_llm_model,
    api_key=settings.openai_api_key,
    temperature=0.0,
)

profile = BrowserProfile(
    headless=False,
    disable_security=True,
    storage_state=storage_state,  # Load cached sessions
    minimum_wait_page_load_time=2.0,
    wait_between_actions=1.0,
    extra_chromium_args=[
        '--disable-blink-features=AutomationControlled',
        '--disable-dev-shm-usage',
        '--no-sandbox',
    ],
)

agent = Agent(
    task=task_prompt,
    llm=llm,
    browser_profile=profile,
    output_model_schema=OutputModel,  # ALWAYS use Pydantic for structured output
    use_vision=not has_cache,  # Use vision for login forms
    max_actions_per_step=12,
    max_failures=3,
)

result = await agent.run(max_steps=max_steps)
```

### 2. Task Prompt Structure (AI: ALWAYS Use This Format)
```python
task = f"""
You are a [specific role]. Your ONLY job is [clear objective].

CRITICAL RULES:
- Stay on [domain] ONLY - do NOT use search engines
- Verify URL contains "[domain]" before proceeding
- Stop after [specific condition]
- If page is empty, WAIT and retry - do NOT navigate away

STEP 1: Navigate to [specific URL with query params]
- Go DIRECTLY to: {direct_url}
- Wait {wait_time} seconds for page load
- If empty, wait another {wait_time} seconds and refresh

STEP 2: Verify Correct Page
- Check URL contains "[domain]"
- Verify page shows [expected content]

STEP 3: Data Collection
- Scroll down {max_scrolls} times
- Wait {wait_time} seconds between scrolls
- Stop after {max_scrolls} scrolls OR {target_count} items

STEP 4: Extract Data (match Pydantic schema exactly)
For each item, extract:
- field_name: description (type, format example)
- url_field: MUST be https://domain.com/... format
- timestamp: Convert relative times ("2h ago") to ISO 8601

WHAT TO SKIP:
- Promoted/sponsored content
- Duplicates
- Invalid URLs

FINAL OUTPUT:
JSON format: {{"items": [...]}}
"""
```

### 3. Pydantic Schema Pattern (AI: Define Before Writing Agent)
```python
from pydantic import BaseModel, Field
from typing import List
from datetime import datetime

class ExtractedItem(BaseModel):
    """Individual item schema - be specific with Field descriptions."""
    item_id: str
    item_url: str
    author_handle: str | None = None
    text: str
    timestamp: str  # Raw timestamp from page
    like_count: int | None = None
    hashtags: List[str] = Field(default_factory=list)
    raw_json: dict | None = None

class ItemBatch(BaseModel):
    """Batch container - this is the output_model_schema."""
    items: List[ExtractedItem]
```

### 4. Session Caching Pattern (AI: ALWAYS Implement This)
```python
import json
from pathlib import Path
from datetime import datetime, timedelta

def _load_session_cache(self) -> dict | None:
    """Load cached session state if available and not expired."""
    if not self.session_cache.exists():
        self.logger.info("No cached session found")
        return None
    
    try:
        with open(self.session_cache, 'r') as f:
            cache_data = json.load(f)
        
        # Check expiration (24 hours)
        cache_time = datetime.fromisoformat(cache_data.get('timestamp', ''))
        if datetime.now() - cache_time > timedelta(hours=24):
            self.logger.info("Session cache expired, will re-authenticate")
            self.session_cache.unlink()
            return None
        
        storage_state = cache_data.get('storage_state')
        if not storage_state or not isinstance(storage_state, dict):
            self.logger.warning("Invalid cache structure")
            self.session_cache.unlink()
            return None
        
        self.logger.info("Loaded cached session")
        return storage_state
        
    except Exception as exc:
        self.logger.warning("Failed to load session cache: %s", exc)
        if self.session_cache.exists():
            self.session_cache.unlink()
        return None

async def _save_session_cache(self, profile: BrowserProfile) -> None:
    """Save current session state to cache."""
    try:
        if hasattr(profile, 'browser') and profile.browser:
            context = profile.browser.contexts[0] if profile.browser.contexts else None
            if context:
                storage_state = await context.storage_state()
                
                cache_data = {
                    'timestamp': datetime.now().isoformat(),
                    'storage_state': storage_state
                }
                
                with open(self.session_cache, 'w') as f:
                    json.dump(cache_data, f, indent=2)
                
                self.logger.info("Saved session cache")
                
    except Exception as exc:
        self.logger.warning("Failed to save session cache: %s", exc)
```

### 5. Result Parsing Pattern (AI: Handle All Result Types)
```python
def _parse_result(self, result) -> Sequence[DomainModel]:
    """Parse agent result into domain models."""
    raw_batch: BatchModel | None = None
    
    # Try multiple extraction methods
    if hasattr(result, 'history') and getattr(result.history, 'structured_output', None):
        candidate = result.history.structured_output
        if isinstance(candidate, BatchModel):
            raw_batch = candidate
    
    if raw_batch is None and hasattr(result, 'final_result'):
        final_value = result.final_result() if callable(result.final_result) else result.final_result
        if isinstance(final_value, BatchModel):
            raw_batch = final_value
        elif isinstance(final_value, dict) and 'items' in final_value:
            raw_batch = BatchModel(**final_value)
    
    if raw_batch is None:
        self.logger.warning("No structured output returned")
        return []
    
    # Parse and validate
    parsed: list[DomainModel] = []
    seen_ids: set[str] = set()
    
    for item in raw_batch.items:
        try:
            # Validate required fields
            if not item.url or not item.url.startswith("http"):
                continue
            if item.id in seen_ids:
                continue
            
            # Transform to domain model
            parsed.append(DomainModel(
                id=item.id,
                url=item.url,
                text=item.text,
                posted_at=self._parse_timestamp(item.timestamp),
                # ... other fields
            ))
            seen_ids.add(item.id)
            
        except Exception as exc:
            self.logger.debug("Failed to parse item %s: %s", item.id, exc)
            continue
    
    self.logger.info("Parsed %d items", len(parsed))
    return parsed
```

### 6. Timestamp Parsing Pattern (AI: Include This Helper)
```python
from datetime import datetime, timedelta, timezone

def _parse_timestamp(self, timestamp: str | None) -> datetime | None:
    """Parse relative timestamps like '2h ago' into datetime."""
    if not timestamp:
        return None
    
    ts = timestamp.strip().lower()
    now = datetime.now(timezone.utc)
    
    try:
        # Handle relative times
        if 's' in ts and any(ch.isdigit() for ch in ts):
            seconds = int(''.join(ch for ch in ts if ch.isdigit()))
            return now - timedelta(seconds=seconds)
        if 'min' in ts or 'm' in ts:
            minutes = int(''.join(ch for ch in ts if ch.isdigit()))
            return now - timedelta(minutes=minutes)
        if 'hour' in ts or 'hr' in ts or 'h' in ts:
            hours = int(''.join(ch for ch in ts if ch.isdigit()))
            return now - timedelta(hours=hours)
        if 'day' in ts or 'd' in ts:
            days = int(''.join(ch for ch in ts if ch.isdigit()))
            return now - timedelta(days=days)
        if 'week' in ts or 'wk' in ts or 'w' in ts:
            weeks = int(''.join(ch for ch in ts if ch.isdigit()))
            return now - timedelta(weeks=weeks)
        
        # Try ISO format
        from dateutil import parser
        parsed = parser.parse(ts)
        if parsed.tzinfo is None:
            parsed = parsed.replace(tzinfo=timezone.utc)
        return parsed.astimezone(timezone.utc)
        
    except Exception:
        return None
```

## Project-Specific Guidelines (AI: Follow These Rules)

### When Writing Browser-Use Adapters:
1. **File Structure**: `app/adapters/{platform}_source.py`
2. **Required Classes**:
   - `ExtractedItem(BaseModel)` - Pydantic schema for raw extraction
   - `ItemBatch(BaseModel)` - Container with `items: List[ExtractedItem]`
   - `{Platform}Collector` - Main collector class with session management
3. **Required Methods**:
   - `async def collect() -> Sequence[DomainModel]`
   - `def _build_task_prompt() -> str`
   - `def _parse_result(result) -> Sequence[DomainModel]`
   - `def _load_session_cache() -> dict | None`
   - `async def _save_session_cache(profile) -> None`
4. **Use Settings**: Always use `get_settings()` for API keys, model names
5. **Use Logger**: Always use `get_logger(self.__class__.__name__)`
6. **Error Handling**: Wrap in try/except, raise custom Error classes

### When Writing CLI Commands:
1. **File Structure**: `app/cli/{platform}_sentiment.py`
2. **Use Typer**: Import `typer` for CLI
3. **Add Commands**: Each command should be a decorated function
4. **Example Reference**: See `app/cli/twitter_sentiment.py`

### Configuration Priority:
1. Use `app/infra/config.py` - `get_settings()`
2. Chrome path: `/Applications/Google Chrome.app/Contents/MacOS/Google Chrome` (macOS)
3. Cache directory: `cache/` (create if not exists)
4. User ID: Always default to 1 (from memory)

### Anti-Patterns to Avoid (AI: NEVER Do These):
- ❌ Don't let agents use search engines when targeting specific sites
- ❌ Don't hardcode credentials in prompts (use settings)
- ❌ Don't skip session caching for authenticated sites
- ❌ Don't set max_steps < 15 (agents need recovery room)
- ❌ Don't forget URL validation in parsed data
- ❌ Don't use infinite scrolling (use fixed scroll counts)
- ❌ Don't create agents without output_model_schema
- ❌ Don't set max_actions_per_step > 10 (causes scroll timeouts)
- ❌ Don't rely on scrolling - extract visible content first
- ❌ Don't use vision mode unnecessarily (slows down agents)

## Critical Learnings from Production Use (AI: MUST Follow)

### Lesson 1: Handling Empty Page Issues
**Problem:** Browser-Use agents often report "empty page" even when page loaded successfully.
**Root Cause:** Screenshot/DOM watchdog timeouts during page rendering.
**Solution:**
```python
# In task prompt, add explicit wait and retry logic:
"""
STEP 1: Navigate to https://example.com
- Wait 10 seconds for the page to fully load
- If page appears empty, wait another 10 seconds and refresh
- Do NOT give up and use search engines
"""

# In BrowserProfile, increase wait times:
profile = BrowserProfile(
    minimum_wait_page_load_time=2.0,  # Minimum 2 seconds
    wait_between_actions=1.0,  # 1 second between actions
)
```

### Lesson 2: Preventing Search Engine Escape
**Problem:** Agents navigate to DuckDuckGo/Google when target site is slow.
**Solution:**
```python
# Add EXPLICIT restrictions in task prompt:
"""
CRITICAL RULES - READ FIRST:
- You MUST stay on TARGET_SITE.com ONLY
- Do NOT use search engines like DuckDuckGo, Google, Bing
- If you see an empty page, WAIT and try again - don't search elsewhere
- Verify URL contains "TARGET_SITE.com" before proceeding
"""

# Also navigate directly to search results URL:
# BAD:  "Go to twitter.com, find search, type query"
# GOOD: "Go to https://twitter.com/search?q=ENCODED_QUERY&f=live"
```

### Lesson 3: Smart Scrolling Strategy
**Problem:** Scroll actions frequently timeout (8s limit).
**Solution - Extract First, Scroll Later:**
```python
# In task prompt:
"""
STEP 3: Extract Visible Content First
- WITHOUT scrolling yet, extract ALL items currently visible
- Most pages show 10-25 items initially

STEP 4: Decide if Scrolling is Needed
- Count extracted items
- If you have TARGET or more → DONE, return results
- If you need more → proceed to scroll

STEP 5: Scroll Carefully (only if needed)
- Scroll down ONE page at a time
- Wait 6 seconds after EACH scroll
- Extract NEW items that appear
- If scroll fails/times out, return what you have so far
"""

# Reduce max_actions_per_step to avoid scroll batching:
agent = Agent(
    max_actions_per_step=6,  # Not 12-18, prevents timeout
)
```

### Lesson 4: Cookie-Based Authentication
**Problem:** Automated login via Browser-Use is unreliable (2FA, verification codes, slow pages).
**Solution - Manual Cookie Export:**
```python
# Create separate CLI command for manual cookie setup:
@app.command()
def platform_login_simple() -> None:
    """Simple cookie-based login - paste cookies from browser."""
    # 1. User logs in manually in their regular browser
    # 2. Export cookies using Cookie-Editor extension
    # 3. Paste JSON into terminal
    # 4. Save to cache/platform_session.json
    
# Benefits:
# - Handles 2FA/verification manually
# - No browser automation headaches
# - Works 100% of the time
# - User controls the login process
```

### Lesson 5: Direct URL Navigation
**Problem:** Multi-step navigation wastes actions and increases failure risk.
**Solution:**
```python
import urllib.parse

# Pre-build URLs with all parameters:
query = "(tsla OR Tesla) min_replies:100"
encoded = urllib.parse.quote(query)
direct_url = f"https://twitter.com/search?q={encoded}&f=live"

# In task prompt:
f"""
STEP 1: Navigate to Search Results
- Go DIRECTLY to: {direct_url}
- This URL has the search query already embedded
- Wait 10 seconds for results to load
"""

# Don't make agent find search bar, type, and submit - too many steps!
```

### Lesson 6: Graceful Degradation
**Problem:** Agents fail completely if any step fails.
**Solution:**
```python
# In task prompt, add fallback instructions:
"""
IMPORTANT:
- If scrolling fails or times out, return whatever posts you've collected so far
- Partial results are better than no results
- Don't retry failed scrolls - just move on
"""

# In parsing code, handle empty results:
if not raw_batch or not raw_batch.items:
    self.logger.warning("No items returned, but this may be partial success")
    return []  # Don't crash, just return empty
```

### Lesson 7: Session Cache Management
**Best Practices:**
```python
# Always check cache validity (24 hours):
def _load_session_cache(self) -> dict | None:
    if not self.session_cache.exists():
        return None
    
    cache_time = datetime.fromisoformat(cache_data['timestamp'])
    if datetime.now() - cache_time > timedelta(hours=24):
        self.session_cache.unlink()  # Delete expired cache
        return None
    
    # Validate structure before using
    if not storage_state or not isinstance(storage_state, dict):
        self.session_cache.unlink()
        return None
    
    return storage_state

# Reduce agent complexity for cached sessions:
has_cache = self._load_session_cache() is not None
max_steps = 10 if has_cache else 15
max_actions = 6 if has_cache else 10
```

### Lesson 8: Task Prompt Structure for Reliability
**Best Practices:**
```python
def _build_task_prompt(self) -> str:
    # 1. Start with role and critical rules
    return f"""
You are a [PLATFORM] data collector. Your ONLY job is to collect [SPECIFIC DATA].

CRITICAL RULES - READ FIRST:
- MUST stay on [PLATFORM].com ONLY - no search engines
- If empty page, WAIT and retry - don't give up
- You have authenticated cookies loaded
- Do NOT navigate away under any circumstances

# 2. Use direct URLs with pre-built queries
STEP 1: Navigate to [PLATFORM]
- Go DIRECTLY to: {pre_built_url_with_query}
- Wait 10 seconds for page load
- If empty, wait 10 more seconds and refresh

# 3. Explicit verification step
STEP 2: Verify Correct Page
- Check URL contains "[PLATFORM].com"
- Verify you see the expected content type
- If wrong page, navigate back

# 4. Extract-first strategy
STEP 3: Extract Visible Content
- Don't scroll yet
- Extract ALL visible items

# 5. Conditional scrolling
STEP 4: Scroll Only If Needed
- If you have {target}+ items → DONE
- Otherwise, scroll carefully with long waits

# 6. Clear output format
FINAL OUTPUT:
Return JSON: {{"items": [...]}}
    """
```

### Lesson 9: Browser Profile Optimization
**For Different Platforms:**
```python
# Twitter/X (strict anti-bot):
profile = BrowserProfile(
    headless=False,
    disable_security=True,
    minimum_wait_page_load_time=2.0,
    wait_between_actions=1.0,
    storage_state=storage_state,
    extra_chromium_args=[
        '--disable-blink-features=AutomationControlled',
        '--disable-dev-shm-usage',
        '--no-sandbox',
        '--disable-web-security',
        '--disable-features=IsolateOrigins,site-per-process',
    ],
)

# Reddit (more lenient):
profile = BrowserProfile(
    headless=False,
    disable_security=True,
    minimum_wait_page_load_time=2.0,
    wait_between_actions=1.0,
    storage_state=storage_state,  # Optional for Reddit
    extra_chromium_args=[
        '--disable-blink-features=AutomationControlled',
        '--no-sandbox',
    ],
)
```

### Lesson 10: CLI User Experience
**Separate Login from Collection:**
```python
# GOOD: Two separate commands
@app.command()
def platform_login_simple():
    """One-time cookie setup."""
    pass

@app.command()
def platform_sentiment():
    """Main collection workflow."""
    # Check for cache, guide user if missing
    if not has_cache:
        console.print("Run platform-login-simple first")
        return
    # Proceed with collection
```

**Add Cache Management:**
```python
# Useful flags:
--check-cache    # Show cache status and expiration
--clear-cache    # Force re-authentication
```

## Reference Examples (AI: Study These Before Writing Code)

### For Session Management:
- Study: `.browser-use-reference/examples/browser/save_cookies.py`
- Study: `app/adapters/twitter_source.py` (session caching)
- Study: `app/cli/twitter_sentiment.py` (twitter-login-simple command)

### For Scrolling:
- Study: `.browser-use-reference/examples/features/scrolling_page.py`
- Study: `app/adapters/reddit_source.py` (extract-first strategy)

### For Structured Output:
- Study: `.browser-use-reference/examples/cloud/03_structured_output.py`
- Study: `app/adapters/twitter_source.py` (lines 21-43)

### For Parallel Agents:
- Study: `.browser-use-reference/examples/custom-functions/parallel_agents.py`

### Complete Working Examples:
- **Twitter** (authenticated): `app/adapters/twitter_source.py`
- **Reddit** (guest): `app/adapters/reddit_source.py`

## Code Generation Checklist (AI: Verify Before Responding)

When generating browser-use code, ensure:
- [ ] Pydantic models defined with Field descriptions
- [ ] Task prompt is structured with STEP 1, STEP 2, etc.
- [ ] Direct URLs provided (no search engine usage)
- [ ] Session caching implemented (_load_session_cache, _save_session_cache)
- [ ] Browser profile has stealth configuration
- [ ] output_model_schema parameter used in Agent
- [ ] Result parsing handles all extraction methods
- [ ] Timestamp parsing included
- [ ] URL validation in parsing
- [ ] Deduplication with seen_ids set
- [ ] Proper error handling with try/except
- [ ] Logger used for info, warning, error
- [ ] Settings imported from config
- [ ] Type hints on all functions
- [ ] Docstrings on classes and key methods

## Quick Snippets (AI: Use These Templates)

### New Social Media Adapter Template:
```python
# See: examples/04_add_new_platform.py for complete example
from browser_use import Agent, ChatOpenAI
from browser_use.browser.profile import BrowserProfile
from pydantic import BaseModel, Field
from app.infra import get_logger, get_settings

class Extracted{Platform}Post(BaseModel):
    # Define schema...
    pass

class {Platform}Collector:
    def __init__(self, query: str):
        self.query = query
        self.settings = get_settings()
        self.logger = get_logger(self.__class__.__name__)
        self.session_cache = Path("cache/{platform}_session.json")
    
    async def collect(self) -> Sequence[DomainModel]:
        # Implement collection...
        pass
```

### CLI Command Template:
```python
import typer
app = typer.Typer()

@app.command()
def {platform}-sentiment(
    query: str = typer.Option("Tesla", help="Search query"),
    target: int = typer.Option(50, help="Target count"),
):
    """Collect sentiment from {Platform}."""
    asyncio.run(collect_{platform}_posts(query, target))
```

## When User Asks to Add New Features (AI: Follow This Flow)

1. **Check existing patterns**: Reference twitter_source.py or reddit_source.py
2. **Define Pydantic schemas first**: Show user the data structure
3. **Write task prompt**: Use STEP 1, STEP 2 format with validation
4. **Implement session caching**: If site requires auth
5. **Add stealth config**: Use BrowserProfile with anti-detection args
6. **Parse and validate**: Handle all result extraction paths
7. **Add CLI wrapper**: In app/cli/ directory
8. **Update README**: Add usage instructions

## Performance Guidelines (AI: Optimize Code)

- Use `gpt-4o-mini` for most tasks (faster, cheaper)
- Reduce max_steps for cached sessions (10-15 vs 20-25)
- Use vision mode only for login: `use_vision=not has_cache`
- Set fixed scroll counts, not infinite scrolling
- Implement deduplication to avoid re-processing

## Error Messages (AI: Provide Helpful Errors)

When errors occur, suggest:
```python
raise {Platform}CollectionError(
    f"Failed to collect posts: {exc}\n"
    f"Tip: Check if Chrome is installed at the correct path.\n"
    f"Tip: Try clearing cache with --clear-cache flag.\n"
    f"Tip: Some sites may require manual login on first run."
) from exc
```

## Common Browser-Use Issues & Solutions (AI: Reference When Debugging)

### Issue 1: "Clean screenshot timed out" Warnings
**Symptom:**
```
WARNING [BrowserSession] 📸 Clean screenshot timed out after 6 seconds
WARNING [BrowserSession] 🔍 DOMWatchdog.on_BrowserStateRequestEvent: Clean screenshot failed
```
**Cause:** Page is loading slowly or has complex JavaScript  
**Solution:** This is usually harmless - agent can still proceed. If it persists:
- Increase `minimum_wait_page_load_time` to 3.0+
- Add explicit `wait: 10` actions in prompt after navigation
- Use `use_vision=False` to skip screenshot processing

### Issue 2: "Scroll X/X failed: Event handler timed out"
**Symptom:**
```
WARNING [tools] Scroll 1/1 failed: Event handler timed out after 8.0s
INFO [tools] 🔍 Scrolled down the page by one page (1328px)
```
**Cause:** Scroll action exceeded 8-second timeout  
**Solution:**
- Reduce `max_actions_per_step` to 6 or less (prevents scroll batching)
- Add 6-second wait AFTER each scroll in prompt
- Use "extract-first, scroll-later" strategy
- Accept partial results if scrolling fails repeatedly

### Issue 3: Agent Navigates to Search Engines
**Symptom:**
```
INFO [Agent] 🎯 Next goal: Use a search engine to find...
INFO [tools] 🔍 Searched Duckduckgo for '...'
```
**Cause:** Agent sees empty page and gives up, tries alternative  
**Solution:**
- Add "CRITICAL RULES" section at top of prompt
- Explicitly forbid search engine use
- Use direct URLs with embedded queries
- Add URL verification step

### Issue 4: "No structured output returned"
**Symptom:**
```
WARNING [TwitterCollector] No structured output returned
```
**Cause:** Agent completed but didn't return Pydantic model  
**Solution:**
- Ensure `output_model_schema` is set in Agent()
- Check prompt includes "Return JSON: {...}" instruction
- Parse result.history.structured_output in multiple ways (see _parse_result methods)
- Increase `max_steps` to give agent more time

### Issue 5: "Browser.close: Connection closed while reading"
**Symptom:**
```
Exception: Browser.close: Connection closed while reading from the driver
```
**Cause:** Browser was closed manually before session could be saved  
**Solution:**
- In manual login workflows, save session BEFORE closing browser
- Add try/except around browser.close()
- Use KeyboardInterrupt handling for Ctrl+C saves
- Or use cookie-export method instead

### Issue 6: Session Cache Not Working
**Symptom:** Agent asks for login every time despite cache file existing  
**Solution:**
```python
# Verify cache structure:
{
  "timestamp": "2025-10-16T14:27:45.855213",
  "storage_state": {
    "cookies": [...],  # Must be array of cookie objects
    "origins": []
  }
}

# Check expiration:
if datetime.now() - cache_time > timedelta(hours=24):
    # Cache expired, need re-auth

# Validate before using:
if not storage_state or not isinstance(storage_state, dict):
    return None  # Invalid cache
```

## Testing (AI: Suggest Testing Steps)

When generating new code, suggest:
1. Test with small target (5 items) first
2. Run with headless=False to watch browser
3. Check parsed output matches Pydantic schema
4. Verify session caching works (run twice, second should be faster)
5. Test with expired cache (delete cache file)
